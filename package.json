{
  "name": "webaudio",
  "version": "1.0.0",
  "description": "write audio streams with javascript functions in the browser",
  "main": "webaudio.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/NHQ/webaudio.git"
  },
  "keywords": [
    "web",
    "audio",
    "DSP"
  ],
  "author": {
    "name": "johnny"
  },
  "license": "MIT",
  "readmeFilename": "README.md",
  "readme": "# webaudio\n\nGenerate audio streams (DSP) with javascript functions in the browser. Compatible with [baudio](https://github.com/substack/baudio) functions. Works on webkits only, including iOS 6 mobile safari (see note below).\n\n```js\nvar  A0 = 440\n,    tau = Math.PI * 2\n\nfunction sineWave(t, i){\n\treturn Math.sin( t * tau * A0 )\n}\n...\n```\n\nYour function will be called with the following arguments:\n\n* Time (float)\n* Sample index (integer)\n* Input sample (float), this will be zero if there is no input. See \"advanced\" use below.\n* // Midi commands: this is not implemented.\n* // Fundamental Frequency: this is not implemented either, just an idea for cases such as effects node chains\n* // Data Object : also not implemented.\n\nAs a helper, this module will write _SAMPLERATE on the window. You cannot change sampleRates in the web audio api as of yet. I tried writing a mock downsampler into webaudio, but it did not sound good, nor bad, enough. \n\nYou function should return a float between [-1, 1]. See examples below.\n\nTo play around with it, do\n\n```bash\nnpm install -g browserify opa\ngit clone https://github.com/NHQ/webaudio\ncd webaudio\nopa\n```\n\nOpen your browser to http://localhost:11001\nEdit entry.js and refresh\n\nsee also [opa](https://github.com/NHQ/opa)\n\n# usage\n\n```bash\nnpm install webaudio\n```\n\nSimple use:\n```js\nvar webaudio = require('webaudio')\n  , tau = Math.PI * 2\n  , frequency = 555\n;\n\nfunction sine(time, i){\n  return Math.sin(time * tau * frequency)\n}\n\nvar channel = webaudio(sine);\n\nchannel.play()\n// later, channel.stop()\n\n```\nFor more advanced use, pass an audioContext as the first argument. You then use the same audio context to connect and use other webaudio functions, or other HTML5 Web Audio API nodes. This module (webaudio) returns a finished ScriptProcessorNode, which you can connect to other nodes, or, to the final audioContext.destination:\n\n```js\nvar webaudio = require('webaudio')\n  , tau = Math.PI * 2\n  , frequency = 555\n  , context = new webkitAudioContext();\n\nfunction sine(time, i){\n  return Math.sin(time * tau * frequency)\n}\n\nfunction gain(time, i, inputSample){\n  return inputSample * 1 / 4 \n}\n\nvar signal = webaudio(context, sine)\nvar gain = webaudio(context, gain)\n\nsignal.connect(gain);\n\npath.connect(context.destination)\n\n/*\nsetTimeout(function(){\n  path.disconnect(); \t// path.stop()\n  path.connect(recorder);\n  recorder.connect(repeater);\n  repeater.connect(audio.destination)\n}, 1000)\n*/\n```\n\n## Mobile Safari\nOn mobile safari webkit (iOS), you can only initiate web audio API sounds from within a user event context (at least the fist time...?). Ergo, channel.play(), or channel.connect(context.destination) must be called from inside an event callback.\n\niOS 5.x does not support the webkit that supports the web audio api.\n\n#Methods\n\n## channel.play()\nStart playing the sound. This connects the node to the master contexts destination for you.\n\n## channel.stop()\nDisconnects the node from the master\n\n## channel.createSample(duration)\nThis helper method will run your function for as long as **duration**, in *samples*.\n\nie, 48000 =~ one second, depending on your sound card\n\nThis returns an [audioBuffer node](http://www.w3.org/TR/webaudio/#AudioBuffer), which are what you use for precision timed samples and loops. So you could for instance write a script that creates 1 second samples for every note value, through your synth function.\n\n\n\n\n\n\n\n\n\n\n\n",
  "_id": "webaudio@1.0.0",
  "_from": "webaudio"
}
